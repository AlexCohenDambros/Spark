{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TDESpark.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axJdjrWfLf2R",
        "outputId": "9966f551-99e2-425c-df68-37d1ae550920"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 25 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 47.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=8bc478136c1d44706f5e8019c4577a853e2cc100e78114522f9278d410fd20ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hu13IaUtLh_9",
        "outputId": "7dde2ba1-869c-4366-cbcc-bc68ac62cab2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkFiles\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]').\\\n",
        "        appName('alex').getOrCreate()\n",
        "        \n",
        "sc = spark.sparkContext\n",
        "\n",
        "sc.addFile('/content/drive/MyDrive/transactions_amostra.csv')\n",
        "\n",
        "# carregando o arquivo\n",
        "rdd = sc.textFile('file://' + SparkFiles.get('transactions_amostra.csv'))\n",
        "\n",
        "# fazendo split de sentenças em palavras\n",
        "palavras = rdd.map(lambda frase: frase.split(';')).filter(lambda l: l[1].isnumeric())"
      ],
      "metadata": {
        "id": "ujNTkQ69LhvF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Questão1 \n",
        "# 1. (Easy) The number of transactions involving Brazil;\n",
        "\n",
        "# primeiramente fazemos um filter pra filtrar todas as linhas em que o 'Brazil' se encontra passando x[0], depois fazemos um map para contar todas as ocorrencias, e por fim somamos tudo usando um reduceByKey.\n",
        "questao1 = palavras.filter(lambda x: 'Brazil' in x[0]).map(lambda x: (x[0], 1)).reduceByKey(lambda x ,y: x + y)\n",
        "questao1.take(5)\n",
        "\n",
        "questao1.coalesce(1).saveAsTextFile('questao1.txt')\n"
      ],
      "metadata": {
        "id": "9SsCW9zoLiDQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "214184d2-10d8-4937-83b9-ec69be09f5d2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Brazil', 54762)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Questão2\n",
        "# 2. (Easy) The number of transactions per year;\n",
        "\n",
        "# primeiramente fazemos um map na coluna 'year' e passando 1 como ocorrencia de cada ano que é encontrado, por fim fazemos somamos tudo usando um reduceByKey para retornar a soma das ocorrencias de todos os anos.\n",
        "questao2 = palavras.map(lambda x: (x[1], 1)).reduceByKey(lambda x ,y: x + y)\n",
        "questao2.take(5)\n",
        "\n",
        "questao2.coalesce(1).saveAsTextFile('questao2.txt')"
      ],
      "metadata": {
        "id": "j0tUjVYhWzn1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d5a46d5-f83b-4940-dfae-0d0e3b138e44"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('2011', 105486),\n",
              " ('1991', 22652),\n",
              " ('2005', 105913),\n",
              " ('1999', 86191),\n",
              " ('2000', 97572)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Questão3 \n",
        "# 3. (Easy) The number of transactions per flow type and year;\n",
        "\n",
        "# primeiramente fazemos um map na coluna ano e na coluna de flow type com uma espaço no meio para separar as strings, e passando 1 como contagem de cada linha econtrada. Por fim fazemos um reduceByKey para realizar o somatório de todas as ocorrencias.\n",
        "questao3 = palavras.map(lambda x: (x[1] + \" \" + x[4], 1)).reduceByKey(lambda x ,y: x + y)\n",
        "questao3.take(5)\n",
        "\n",
        "questao3.coalesce(1).saveAsTextFile('questao3.txt')"
      ],
      "metadata": {
        "id": "r0ynT5yeNcrN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dce724d1-743b-4f9c-a4e3-579fb6ff3b05"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('1997 Import', 47599),\n",
              " ('2000 Export', 33582),\n",
              " ('2014 Re-Export', 5573),\n",
              " ('2013 Export', 36090),\n",
              " ('1998 Re-Export', 3017)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Questão 4\n",
        "# 4. (Easy) The average of commodity values per year;\n",
        "\n",
        "# primeiramente fazemos um map na coluna ano e na coluna \"trade_usd\", depois fazemos um groupByKey para agrupar as chaves e os valores e por fim fazer novamente um map passando pelas chaves e realizando uma média entre os valores\n",
        "questao4 = palavras.map(lambda x: (x[1], float(x[5]))).groupByKey()\\\n",
        ".map(lambda x: (x[0], sum(list(x[1])) / len(list(x[1]))))\n",
        "\n",
        "questao4.take(5)\n",
        "questao4.saveAsTextFile('questao4.txt')"
      ],
      "metadata": {
        "id": "xFdGsuCUOHpy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c558a1e-a134-49c4-9496-6b24773a6a90"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('2011', 37056771.709060915),\n",
              " ('1991', 12069165.192609925),\n",
              " ('2005', 17458172.20780263),\n",
              " ('1999', 9328194.404264946),\n",
              " ('2000', 12780250.522332227)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Questão 5\n",
        "# (Easy) The average price of commodities per unit type, year, and category in the export flow in Brazil;\n",
        "\n",
        "# primeiramente fazer um filter pra pegar apenas o pais \"Brazil\" e o flow type como \"Export\", depois fazemos um map pegando o unit, year, category, flowType e pais\n",
        "# depois fazemos um groupByKey para agrupar as chaves e os valores e por fim um ultimpo map passando pelas chaves e realizando as medias dos valores.\n",
        "\n",
        "questao5 = palavras.filter(lambda x: x[0] == \"Brazil\" and x[4] == \"Export\")\n",
        "finalQT5 = questao5.map(lambda x: ((x[7] + \"\" + x[1] + \" \" + x[9] + \" \" + x[4] + \" \" + x[0]), float(x[5]))).groupByKey()\\\n",
        ".map(lambda x: (x[0], sum(list(x[1])) / len(list(x[1]))))\n",
        "\n",
        "finalQT5.take(10)\n",
        "finalQT5.saveAsTextFile('questao5.txt')"
      ],
      "metadata": {
        "id": "-Gnvy69hhO7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db319eb2-d662-473a-b6d8-2a1caa3a59b0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Weight in kilograms2016 62_articles_of_apparel_accessories_not_knit_or_crochet Export Brazil',\n",
              "  244220.66666666666),\n",
              " ('Weight in kilograms1990 87_vehicles_other_than_railway_tramway Export Brazil',\n",
              "  37357374.36363637),\n",
              " ('Weight in kilograms2002 78_lead_and_articles_thereof Export Brazil',\n",
              "  312033.0),\n",
              " ('Weight in kilograms1992 08_edible_fruit_nuts_peel_of_citrus_fruit_melons Export Brazil',\n",
              "  13613885.857142856),\n",
              " ('Weight in kilograms2014 18_cocoa_and_cocoa_preparations Export Brazil',\n",
              "  76505492.0),\n",
              " ('Weight in kilograms2005 48_paper_paperboard_articles_of_pulp_paper_and_board Export Brazil',\n",
              "  6539302.84),\n",
              " ('Weight in kilograms1992 72_iron_and_steel Export Brazil',\n",
              "  29707629.604651164),\n",
              " ('Weight in kilograms1998 40_rubber_and_articles_thereof Export Brazil',\n",
              "  15267148.933333334),\n",
              " ('Weight in kilograms2000 03_fish_crustaceans_molluscs_aquatic_invertebrates_ne Export Brazil',\n",
              "  2449754.9545454546),\n",
              " ('Weight in kilograms2003 82_tools_implements_cutlery_etc_of_base_metal Export Brazil',\n",
              "  3362228.1666666665)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Questao 6 \n",
        "# (Medium) The maximum, minimum, and mean transaction price per unit type and year;\n",
        "\n",
        "# primeiramente fazemos um map passando pela 'quantity_name', 'year' e passando como valor a 'trade_usd', depois fazemos um groupByKey para agrupar as chaves e os valores\n",
        "# e por fim um novo map printando o valor minimo encontrado, maximo e a média.\n",
        "\n",
        "questao6 = palavras.map(lambda x: ((x[7], x[1]), float(x[5]))).groupByKey().map(lambda x: (x[0], [min(x[1]), max(x[1]), sum(x[1]) / len(x[1])]))\n",
        "questao6.take(5)\n",
        "\n",
        "questao6.saveAsTextFile('questao6.txt')"
      ],
      "metadata": {
        "id": "dOafZ5jOzJWm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "266313ca-aec2-4db2-ebf4-9d7e78163596"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('Weight in kilograms', '2016'), [1.0, 56865611547.0, 27962304.5069153]),\n",
              " (('Weight in kilograms', '2010'), [1.0, 105814257878.0, 28822456.939314935]),\n",
              " (('Weight in kilograms', '1990'), [1.0, 4480755200.0, 10720973.53031925]),\n",
              " (('Weight in kilograms', '2014'), [1.0, 359475936313.0, 41934737.83217836]),\n",
              " (('Weight in kilograms', '2006'), [1.0, 262924347374.0, 22895231.261773862])]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Questao 7\n",
        "# (Hard) The most commercialized commodity (summing the quantities) in 2016, per flow type.\n",
        "\n",
        "# primeiramente filtramos as linhas que possuem 2016 como ano, depois fazemos uma map pela coluna commodity e por flowtype, passando como valor a coluna de quantity\n",
        "# depois fazemos um groupByKey para agrupar as chaves e os valores\n",
        "# depois fazemos um novo map pegando o flow type na posicao x[0][1], e a soma dos valores na quantity\n",
        "# por fim fazemos um reduceByKey com um if para pegar os maiores valores de cada flow type.\n",
        "\n",
        "questao7 = palavras.filter(lambda x: '2016' in x[1]).map(lambda x: ((x[3], x[4]), float(x[8]))).groupByKey().map(lambda x: (x[0][1], (sum(list(x[1])), x[0][0]))).reduceByKey(lambda x, y: x if x > y else y)\n",
        "questao7.take(5)\n",
        "\n",
        "questao7.saveAsTextFile('questao7.txt')"
      ],
      "metadata": {
        "id": "bLf1HtHx05J9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6267e1c2-f401-4b99-c8ef-c61c19889fb7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Re-Export',\n",
              "  (1261968000.0, 'Safety razor blades, including blanks in strips')),\n",
              " ('Export',\n",
              "  (699847369665.0, 'Ice, snow and potable water not sweetened or flavoure')),\n",
              " ('Re-Import',\n",
              "  (38774873.0, 'Chem wood pulp, soda/sulphate, non-conifer, bleached')),\n",
              " ('Import',\n",
              "  (1073469712878.0, 'Iron ore, concentrate, not iron pyrites,unagglomerate'))]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}